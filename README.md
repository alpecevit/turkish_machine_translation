# Turkish Macine Translation
In this project I have built an attention model to translate texts from English to Turkish. The results of the model show that there is some overfitting happening which points out to the fact that the model needs further training. What can be done to overcome the the overfitting problem?
  - Using a larger dataset
  - Using a dataset with longer sequences
  - Using a corpus with more target embeddings (in this case Turkish word embeddings) i.e., words

# Related Sources and Acknowledgements
  - The link for the dataset can be found from [here](https://www.manythings.org/anki/). You can use any other dataset for your input and target languages to train your own machine translation projects.
  - Special thanks to this [repo](https://github.com/inzva/Turkish-GloVe) for sharing word embeddings in Turkish. 
  - The link for English embeddings can be found from [here](https://nlp.stanford.edu/projects/glove/).
